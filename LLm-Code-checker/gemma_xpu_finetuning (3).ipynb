{
 "cells": [
  {
   "cell_type": "raw",
   "id": "8f15b0f9-3b7b-42cb-81fe-4ada30017540",
   "metadata": {},
   "source": [
    "SPDX-License-Identifier: Apache-2.0 \n",
    "Copyright (c) 2023, Rahul Unnikrishnan Nair <rahul.unnikrishnan.nair@intel.com>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7f92c8-c19f-4c6c-a5cd-e0a9bfee98a5",
   "metadata": {},
   "source": [
    "## Finetuning Google's Gemma Model on Intel Max Series GPUs 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b60229a-c6c5-45f8-8136-9f33e73c3cdc",
   "metadata": {},
   "source": [
    "Welcome to this exciting journey where we'll dive into the world of finetuning large language models (LLMs) using Intel® Data Center GPU Max Series! 🌟 \n",
    "\n",
    "In this notebook, we'll be working with Google's Gemma model and optimizing it for a specific task using the Intel Max 1550 GPU. 💪\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60f0438-5849-4e76-a048-5ec82d152b66",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "In this notebook, you will learn how to fine-tune a large language model (Google's Gemma) using Intel Max Series GPUs (XPUs) for a specific task. The notebook covers the following key points:\n",
    "\n",
    "1. Setting up the environment and optimizing it for Intel GPUs\n",
    "2. Initializing the XPU and configuring LoRA settings for efficient fine-tuning\n",
    "3. Loading the pre-trained Gemma model and testing its performance\n",
    "4. Preparing a diverse dataset of question-answer pairs covering various domains\n",
    "5. Fine-tuning the model using the Hugging Face `Trainer` class\n",
    "6. Evaluating the fine-tuned model on a test dataset\n",
    "7. Saving and loading the fine-tuned model for future use\n",
    "\n",
    "\n",
    "The notebook demonstrates how fine-tuning can enhance a model's performance on a diverse range of topics, making it more versatile and applicable to various domains. You will gain insights into the process of creating a **task-specific model** that can provide accurate and relevant responses to a wide range of questions.\n",
    "</br>\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20f2282-1206-468a-8688-82d31556d88d",
   "metadata": {},
   "source": [
    "#### Step 1: Setting Up the Environment 🛠️\n",
    "\n",
    "First things first, let's get our environment ready! We'll install all the necessary packages, including the Hugging Face `transformers` library, `datasets` for easy data loading, `wandb` for experiment tracking, and a few others. 📦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e8af2b6-ef07-4046-a603-dbcc891c441f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers>=4.38.* in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (4.41.2)\n",
      "Requirement already satisfied: filelock in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from transformers>=4.38.*) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from transformers>=4.38.*) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from transformers>=4.38.*) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from transformers>=4.38.*) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from transformers>=4.38.*) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from transformers>=4.38.*) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from transformers>=4.38.*) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from transformers>=4.38.*) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from transformers>=4.38.*) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from transformers>=4.38.*) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers>=4.38.*) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers>=4.38.*) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from requests->transformers>=4.38.*) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from requests->transformers>=4.38.*) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from requests->transformers>=4.38.*) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from requests->transformers>=4.38.*) (2024.2.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets>=2.18.* in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (2.20.0)\n",
      "Requirement already satisfied: filelock in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from datasets>=2.18.*) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from datasets>=2.18.*) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from datasets>=2.18.*) (15.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from datasets>=2.18.*) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from datasets>=2.18.*) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from datasets>=2.18.*) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from datasets>=2.18.*) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from datasets>=2.18.*) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from datasets>=2.18.*) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from datasets>=2.18.*) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets>=2.18.*) (2023.12.2)\n",
      "Requirement already satisfied: aiohttp in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from datasets>=2.18.*) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from datasets>=2.18.*) (0.23.4)\n",
      "Requirement already satisfied: packaging in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from datasets>=2.18.*) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from datasets>=2.18.*) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from aiohttp->datasets>=2.18.*) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from aiohttp->datasets>=2.18.*) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from aiohttp->datasets>=2.18.*) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from aiohttp->datasets>=2.18.*) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from aiohttp->datasets>=2.18.*) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from aiohttp->datasets>=2.18.*) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from huggingface-hub>=0.21.2->datasets>=2.18.*) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from requests>=2.32.2->datasets>=2.18.*) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from requests>=2.32.2->datasets>=2.18.*) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from requests>=2.32.2->datasets>=2.18.*) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from requests>=2.32.2->datasets>=2.18.*) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from pandas->datasets>=2.18.*) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from pandas->datasets>=2.18.*) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from pandas->datasets>=2.18.*) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.18.*) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wandb>=0.16.* in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (0.17.2)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from wandb>=0.16.*) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from wandb>=0.16.*) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from wandb>=0.16.*) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from wandb>=0.16.*) (4.2.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.15.0 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from wandb>=0.16.*) (4.25.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from wandb>=0.16.*) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from wandb>=0.16.*) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from wandb>=0.16.*) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from wandb>=0.16.*) (2.6.0)\n",
      "Requirement already satisfied: setproctitle in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from wandb>=0.16.*) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from wandb>=0.16.*) (69.1.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from wandb>=0.16.*) (4.9.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb>=0.16.*) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.16.*) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb>=0.16.*) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb>=0.16.*) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb>=0.16.*) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb>=0.16.*) (2024.2.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.16.*) (5.0.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: trl>=0.7.11 in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (0.9.4)\n",
      "Requirement already satisfied: torch>=1.4.0 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from trl>=0.7.11) (2.1.0.post0+cxx11.abi)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from trl>=0.7.11) (4.41.2)\n",
      "Requirement already satisfied: numpy>=1.18.2 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from trl>=0.7.11) (1.26.4)\n",
      "Requirement already satisfied: accelerate in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from trl>=0.7.11) (0.31.0)\n",
      "Requirement already satisfied: datasets in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from trl>=0.7.11) (2.20.0)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from trl>=0.7.11) (0.8.4)\n",
      "Requirement already satisfied: filelock in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from torch>=1.4.0->trl>=0.7.11) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from torch>=1.4.0->trl>=0.7.11) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from torch>=1.4.0->trl>=0.7.11) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from torch>=1.4.0->trl>=0.7.11) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from torch>=1.4.0->trl>=0.7.11) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from torch>=1.4.0->trl>=0.7.11) (2023.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from transformers>=4.31.0->trl>=0.7.11) (0.23.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from transformers>=4.31.0->trl>=0.7.11) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from transformers>=4.31.0->trl>=0.7.11) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from transformers>=4.31.0->trl>=0.7.11) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from transformers>=4.31.0->trl>=0.7.11) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from transformers>=4.31.0->trl>=0.7.11) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from transformers>=4.31.0->trl>=0.7.11) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from transformers>=4.31.0->trl>=0.7.11) (4.66.4)\n",
      "Requirement already satisfied: docstring-parser>=0.14.1 in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from tyro>=0.5.11->trl>=0.7.11) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from tyro>=0.5.11->trl>=0.7.11) (13.7.1)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from tyro>=0.5.11->trl>=0.7.11) (1.7.1)\n",
      "Requirement already satisfied: eval-type-backport>=0.1.3 in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from tyro>=0.5.11->trl>=0.7.11) (0.2.0)\n",
      "Requirement already satisfied: psutil in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from accelerate->trl>=0.7.11) (5.9.8)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from datasets->trl>=0.7.11) (15.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from datasets->trl>=0.7.11) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from datasets->trl>=0.7.11) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from datasets->trl>=0.7.11) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from datasets->trl>=0.7.11) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from datasets->trl>=0.7.11) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from datasets->trl>=0.7.11) (3.9.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from aiohttp->datasets->trl>=0.7.11) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from aiohttp->datasets->trl>=0.7.11) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from aiohttp->datasets->trl>=0.7.11) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from aiohttp->datasets->trl>=0.7.11) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from aiohttp->datasets->trl>=0.7.11) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from aiohttp->datasets->trl>=0.7.11) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from requests->transformers>=4.31.0->trl>=0.7.11) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from requests->transformers>=4.31.0->trl>=0.7.11) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from requests->transformers>=4.31.0->trl>=0.7.11) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from requests->transformers>=4.31.0->trl>=0.7.11) (2024.2.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl>=0.7.11) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl>=0.7.11) (2.17.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from jinja2->torch>=1.4.0->trl>=0.7.11) (2.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from pandas->datasets->trl>=0.7.11) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from pandas->datasets->trl>=0.7.11) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from pandas->datasets->trl>=0.7.11) (2023.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from sympy->torch>=1.4.0->trl>=0.7.11) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl>=0.7.11) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl>=0.7.11) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: peft>=0.9.0 in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (0.11.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from peft>=0.9.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from peft>=0.9.0) (23.2)\n",
      "Requirement already satisfied: psutil in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from peft>=0.9.0) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from peft>=0.9.0) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from peft>=0.9.0) (2.1.0.post0+cxx11.abi)\n",
      "Requirement already satisfied: transformers in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from peft>=0.9.0) (4.41.2)\n",
      "Requirement already satisfied: tqdm in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from peft>=0.9.0) (4.66.4)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from peft>=0.9.0) (0.31.0)\n",
      "Requirement already satisfied: safetensors in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from peft>=0.9.0) (0.4.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from peft>=0.9.0) (0.23.4)\n",
      "Requirement already satisfied: filelock in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft>=0.9.0) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft>=0.9.0) (2023.12.2)\n",
      "Requirement already satisfied: requests in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft>=0.9.0) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft>=0.9.0) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from torch>=1.13.0->peft>=0.9.0) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from torch>=1.13.0->peft>=0.9.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from torch>=1.13.0->peft>=0.9.0) (3.1.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from transformers->peft>=0.9.0) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from transformers->peft>=0.9.0) (0.19.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from jinja2->torch>=1.13.0->peft>=0.9.0) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft>=0.9.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft>=0.9.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft>=0.9.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft>=0.9.0) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from sympy->torch>=1.13.0->peft>=0.9.0) (1.3.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: accelerate>=0.28.* in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (0.31.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from accelerate>=0.28.*) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from accelerate>=0.28.*) (23.2)\n",
      "Requirement already satisfied: psutil in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from accelerate>=0.28.*) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from accelerate>=0.28.*) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from accelerate>=0.28.*) (2.1.0.post0+cxx11.abi)\n",
      "Requirement already satisfied: huggingface-hub in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from accelerate>=0.28.*) (0.23.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from accelerate>=0.28.*) (0.4.2)\n",
      "Requirement already satisfied: filelock in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from torch>=1.10.0->accelerate>=0.28.*) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from torch>=1.10.0->accelerate>=0.28.*) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from torch>=1.10.0->accelerate>=0.28.*) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from torch>=1.10.0->accelerate>=0.28.*) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from torch>=1.10.0->accelerate>=0.28.*) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from torch>=1.10.0->accelerate>=0.28.*) (2023.12.2)\n",
      "Requirement already satisfied: requests in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from huggingface-hub->accelerate>=0.28.*) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from huggingface-hub->accelerate>=0.28.*) (4.66.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from jinja2->torch>=1.10.0->accelerate>=0.28.*) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate>=0.28.*) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate>=0.28.*) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate>=0.28.*) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate>=0.28.*) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/intel/oneapi/intelpython/envs/pytorch-gpu/lib/python3.9/site-packages (from sympy->torch>=1.10.0->accelerate>=0.28.*) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import site\n",
    "import os\n",
    "\n",
    "# Install the required packages\n",
    "!{sys.executable} -m pip install --upgrade  \"transformers>=4.38.*\"\n",
    "!{sys.executable} -m pip install --upgrade  \"datasets>=2.18.*\"\n",
    "!{sys.executable} -m pip install --upgrade \"wandb>=0.16.*\"\n",
    "!{sys.executable} -m pip install --upgrade \"trl>=0.7.11\"\n",
    "!{sys.executable} -m pip install --upgrade \"peft>=0.9.0\"\n",
    "!{sys.executable} -m pip install --upgrade \"accelerate>=0.28.*\"\n",
    "\n",
    "# Get the site-packages directory\n",
    "site_packages_dir = site.getsitepackages()[0]\n",
    "\n",
    "# add the site pkg directory where these pkgs are insalled to the top of sys.path\n",
    "if not os.access(site_packages_dir, os.W_OK):\n",
    "    user_site_packages_dir = site.getusersitepackages()\n",
    "    if user_site_packages_dir in sys.path:\n",
    "        sys.path.remove(user_site_packages_dir)\n",
    "    sys.path.insert(0, user_site_packages_dir)\n",
    "else:\n",
    "    if site_packages_dir in sys.path:\n",
    "        sys.path.remove(site_packages_dir)\n",
    "    sys.path.insert(0, site_packages_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d52c0da-f19e-4655-8164-33268ad1ab76",
   "metadata": {},
   "source": [
    "We'll now make sure to optimize our environment for the Intel GPU by setting the appropriate environment variables and configuring the number of cores and threads. This will ensure we get the best performance out of our hardware! ⚡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ff51abb-9ca0-4243-b426-6e6dc407e9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of physical cores: 96\n",
      "Number of cores per socket: 48\n",
      "OpenMP environment variables:\n",
      "  - OMP_NUM_THREADS: 96\n",
      "  - OMP_PROC_BIND: close\n",
      "  - OMP_PLACES: cores\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "num_physical_cores = psutil.cpu_count(logical=False)\n",
    "num_cores_per_socket = num_physical_cores // 2\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n",
    "#HF_TOKEN = os.environ[\"HF_TOKEN\"]\n",
    "\n",
    "# Set the LD_PRELOAD environment variable\n",
    "ld_preload = os.environ.get(\"LD_PRELOAD\", \"\")\n",
    "conda_prefix = os.environ.get(\"CONDA_PREFIX\", \"\")\n",
    "# Improve memory allocation performance, if tcmalloc is not available, please comment this line out\n",
    "os.environ[\"LD_PRELOAD\"] = f\"{ld_preload}:{conda_prefix}/lib/libtcmalloc.so\"\n",
    "# Reduce the overhead of submitting commands to the GPU\n",
    "os.environ[\"SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS\"] = \"1\"\n",
    "# reducing memory accesses by fusing SDP ops\n",
    "os.environ[\"ENABLE_SDP_FUSION\"] = \"1\"\n",
    "# set openMP threads to number of physical cores\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(num_physical_cores)\n",
    "# Set the thread affinity policy\n",
    "os.environ[\"OMP_PROC_BIND\"] = \"close\"\n",
    "# Set the places for thread pinning\n",
    "os.environ[\"OMP_PLACES\"] = \"cores\"\n",
    "\n",
    "print(f\"Number of physical cores: {num_physical_cores}\")\n",
    "print(f\"Number of cores per socket: {num_cores_per_socket}\")\n",
    "print(f\"OpenMP environment variables:\")\n",
    "print(f\"  - OMP_NUM_THREADS: {os.environ['OMP_NUM_THREADS']}\")\n",
    "print(f\"  - OMP_PROC_BIND: {os.environ['OMP_PROC_BIND']}\")\n",
    "print(f\"  - OMP_PLACES: {os.environ['OMP_PLACES']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eeb60a-38e8-4c8e-9d92-57c491588f83",
   "metadata": {},
   "source": [
    "___\n",
    "#### Step 2: Initializing the XPU and monitoring GPU memory in realtime 🎮\n",
    "\n",
    "Next, we'll initialize the Intel Max 1550 GPU, which is referred to as an XPU. We'll use the `intel_extension_for_pytorch` library to seamlessly integrate XPU namespace with. 🤝\n",
    "\n",
    "##### 👀 GPU Memory Monitoring 👀\n",
    "\n",
    "To keep track of the Intel Max 1550 GPU (XPU) memory usage throughout this notebook, please refer to the cell below. It displays the current memory usage and updates every 5 seconds, providing you with real-time information about the GPU's memory consumption. 📊\n",
    "\n",
    "The memory monitoring cell displays the following information:\n",
    "\n",
    "- XPU Device Name: The name of the Intel Max 1550 GPU being used.\n",
    "- Reserved Memory: The amount of memory currently reserved by the GPU.\n",
    "- Allocated Memory: The amount of memory currently allocated by the GPU.\n",
    "- Max Reserved Memory: The maximum amount of memory that has been reserved by the GPU.\n",
    "- Max Allocated Memory: The maximum amount of memory that has been allocated by the GPU.\n",
    "\n",
    "Keep an eye on this cell to monitor the GPU memory usage as you progress through the notebook. If you need to check the current memory usage at any point, simply scroll down to the memory monitoring cell for a quick reference. 👇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "577cc356-6cec-411e-b258-22d0e7eb9202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>XPU (Intel(R) Data Center GPU Max 1100) :: Memory: Reserved=30.008 GB, Allocated=29.009 GB, Max Reserved=30.008 GB, Max Allocated=29.651 GB</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import asyncio\n",
    "import threading\n",
    "import torch\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import torch\n",
    "import intel_extension_for_pytorch as ipex\n",
    "\n",
    "if torch.xpu.is_available():\n",
    "    torch.xpu.empty_cache()\n",
    "    \n",
    "    def get_memory_usage():\n",
    "        memory_reserved = round(torch.xpu.memory_reserved() / 1024**3, 3)\n",
    "        memory_allocated = round(torch.xpu.memory_allocated() / 1024**3, 3)\n",
    "        max_memory_reserved = round(torch.xpu.max_memory_reserved() / 1024**3, 3)\n",
    "        max_memory_allocated = round(torch.xpu.max_memory_allocated() / 1024**3, 3)\n",
    "        return memory_reserved, memory_allocated, max_memory_reserved, max_memory_allocated\n",
    "   \n",
    "    def print_memory_usage():\n",
    "        device_name = torch.xpu.get_device_name()\n",
    "        print(f\"XPU Name: {device_name}\")\n",
    "        memory_reserved, memory_allocated, max_memory_reserved, max_memory_allocated = get_memory_usage()\n",
    "        memory_usage_text = f\"XPU Memory: Reserved={memory_reserved} GB, Allocated={memory_allocated} GB, Max Reserved={max_memory_reserved} GB, Max Allocated={max_memory_allocated} GB\"\n",
    "        print(f\"\\r{memory_usage_text}\", end=\"\", flush=True)\n",
    "    \n",
    "    async def display_memory_usage(output):\n",
    "        device_name = torch.xpu.get_device_name()\n",
    "        output.update(HTML(f\"<p>XPU Name: {device_name}</p>\"))\n",
    "        while True:\n",
    "            memory_reserved, memory_allocated, max_memory_reserved, max_memory_allocated = get_memory_usage()\n",
    "            memory_usage_text = f\"XPU ({device_name}) :: Memory: Reserved={memory_reserved} GB, Allocated={memory_allocated} GB, Max Reserved={max_memory_reserved} GB, Max Allocated={max_memory_allocated} GB\"\n",
    "            output.update(HTML(f\"<p>{memory_usage_text}</p>\"))\n",
    "            await asyncio.sleep(5)\n",
    "    \n",
    "    def start_memory_monitor(output):\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "        loop.create_task(display_memory_usage(output))\n",
    "        thread = threading.Thread(target=loop.run_forever)\n",
    "        thread.start()    \n",
    "    output = display(display_id=True)\n",
    "    start_memory_monitor(output)\n",
    "else:\n",
    "    print(\"XPU device not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bca28ce-12b7-42dc-afba-e90c65b029c8",
   "metadata": {},
   "source": [
    "___\n",
    "#### Step 3: Configuring the LoRA Settings 🎛️\n",
    "\n",
    "To finetune our Gemma model efficiently, we'll use the LoRA (Low-Rank Adaptation) technique. \n",
    "\n",
    "LoRA allows us to adapt the model to our specific task by training only a small set of additional parameters. This greatly reduces the training time and memory requirements! ⏰\n",
    "\n",
    "We'll define the LoRA configuration, specifying the rank (`r`) and the target modules we want to adapt. 🎯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0876949a-2abc-482e-beed-01ec20eace82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    # could use q, v and 0 projections as well and comment out the rest\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \n",
    "                    \"v_proj\", \"k_proj\", \n",
    "                    \"gate_proj\", \"up_proj\",\n",
    "                    \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b89c13a-d922-4df0-a334-9db67126e292",
   "metadata": {},
   "source": [
    "___\n",
    "#### Step 4: Loading the Gemma Model 🤖\n",
    "\n",
    "Now, let's load the Gemma model using the Hugging Face `AutoModelForCausalLM` class. We'll also load the corresponding tokenizer to preprocess our input data. The model will be moved to the XPU for efficient training. 💪"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c8af5b-f8d6-4d40-8b12-90e254d715b3",
   "metadata": {},
   "source": [
    "> Note: Before running this notebook, please ensure you have read and agreed to the [Gemma Terms of Use](https://ai.google.dev/gemma/terms). You'll need to visit the Gemma model card on the Hugging Face Hub, accept the usage terms, and generate an access token with write permissions. This token will be required to load the model and push your finetuned version back to the Hub.\n",
    "\n",
    "To create an access token:\n",
    "1. Go to your Hugging Face account settings.\n",
    "2. Click on \"Access Tokens\" in the left sidebar.\n",
    "3. Click on the \"New token\" button.\n",
    "4. Give your token a name, select the desired permissions (make sure to include write access), and click \"Generate\".\n",
    "5. Copy the generated token and keep it secure. You'll use this token to authenticate when loading the model.\n",
    "\n",
    "Make sure to follow these steps to comply with the terms of use and ensure a smooth finetuning experience. If you have any questions or concerns, please refer to the official Gemma documentation or reach out to the Hugging Face community for assistance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19af72a5-9b23-4fe5-8ca8-bf45c7bdf7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d259a27f-d6bf-473f-a485-a056381fcdb2",
   "metadata": {},
   "source": [
    "Now that you have logged in , let's load the model using transformers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fef5d77e-e2b4-4944-9906-655a1b030da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: xpu:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc138e9c2b34cf48e898864f177d878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "USE_CPU = False\n",
    "device = \"xpu:0\" if torch.xpu.is_available() else \"cpu\"\n",
    "if USE_CPU:\n",
    "    device = \"cpu\"\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "model_id = \"Qwen/CodeQwen1.5-7B-Chat\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Set padding side to the right to ensure proper attention masking during fine-tuning\n",
    "tokenizer.padding_side = \"right\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "# Disable caching mechanism to reduce memory usage during fine-tuning\n",
    "model.config.use_cache = False\n",
    "# Configure the model's pre-training tensor parallelism degree to match the fine-tuning setup\n",
    "model.config.pretraining_tp = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38530297-4387-4420-98cc-fd9e9c2ba59a",
   "metadata": {},
   "source": [
    "___\n",
    "#### Step 5: Testing the Model 🧪\n",
    "\n",
    "Before we start finetuning, let's test the Gemma model on a sample input to see how it performs out-of-the-box. We'll generate some responses bsaed on a few questions in the `test_inputs` list below. 🌿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91cd310c-0e10-4d5b-be0e-5baca18028ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response: {\n",
      "  \"Score\": \"70\",\n",
      "  \"Evaluation feedback\": \"The code is good, but the regex_stat function has some bugs. The function is not case-insensitive as requested, and it uses the wrong regex syntax to escape the token. \",\n",
      "  \"Suggestion\": \"Ensure that the regex_stat function is case-insensitive and use the correct syntax to escape the token. \",\n",
      "  \"Total_Score\": \"100\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Example prompt and messages for generating response\n",
    "prompt = \"Grade my code against requirements and provide feedback for each wrong step and right step \"\n",
    "code = \"\"\"\n",
    "import re\n",
    "\n",
    "def regex_stat(file_path):\n",
    "    # Read text from file\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    while True:\n",
    "        token = input(\"Enter a token to search (or 'quit' to exit): \").strip()\n",
    "        \n",
    "        if token.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        # Use re.findall to find all occurrences of the token in the text\n",
    "        occurrences = re.findall(r'\\b{}\\b'.format(re.escape(token)), text, flags=re.IGNORECASE)\n",
    "        \n",
    "        print(f\"Token '{token}' appears {len(occurrences)} times in the text.\")\n",
    "\n",
    "def regex_substitute(file_path):\n",
    "    # Read text from file\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    while True:\n",
    "        old_token = input(\"Enter the token to replace (or 'quit' to exit): \").strip()\n",
    "        \n",
    "        if old_token.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        new_token = input(f\"Enter the new token to substitute '{old_token}' with: \").strip()\n",
    "\n",
    "        # Use re.sub to substitute all occurrences of old_token with new_token\n",
    "        modified_text = re.sub(r'\\b{}\\b'.format(re.escape(old_token)), new_token, text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Print the modified text\n",
    "        print(\"Modified text:\")\n",
    "        print(modified_text)\n",
    "        \n",
    "        # Optionally, write the modified text back to the file\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(modified_text)\n",
    "            print(\"Changes saved to file.\")\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = 'sample.txt'  # Replace with the relative path to your text file\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\nChoose an option:\")\n",
    "        print(\"1. Search for a token (regex_stat)\")\n",
    "        print(\"2. Substitute tokens (regex_substitute)\")\n",
    "        print(\"3. Quit\")\n",
    "        choice = input(\"Enter your choice (1/2/3): \").strip()\n",
    "\n",
    "        if choice == '1':\n",
    "            regex_stat(file_path)\n",
    "        elif choice == '2':\n",
    "            regex_substitute(file_path)\n",
    "        elif choice == '3':\n",
    "            print(\"Exiting program.\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid choice. Please enter 1, 2, or 3.\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "requir =\"\"\"\n",
    "Task -\n",
    "In this homework, you will be creating two functions: \n",
    "1) allows a user to enter a token (word) and\n",
    "then uses regular expressions to create stats about the token in the text, \n",
    "2) allows a user to substitute\n",
    "new tokens for old tokens (via regular expressions).\n",
    "\n",
    "Coding requirements -\n",
    "• You must use regular expressions in python to accomplish the above task. (re library)\n",
    "o Hint: re.sub\n",
    "• You must not split the text into individual words (you will lose points if you do)\n",
    "• Your function must take in a relative path to a filename and use the text from that file for\n",
    "the search and substitute.\n",
    "• Your function should loop until the user enters “quit”, allowing multiple substitutions to be\n",
    "made.\n",
    "\n",
    "Grading Rubrik -\n",
    "Assignment will be graded as follows:\n",
    "Description Points\n",
    "Code Runs 10\n",
    "Regex_stat Implementation 40\n",
    "Regex_substitute Implementation 40\n",
    "Code (Comments, functions, cleanliness, readability) 10\n",
    "Total: 100\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "outcome_format = \"\"\"\n",
    "{\n",
    "  \"Score\": \"{Score}\",\n",
    "  \"Evaluation feedback\": \"{Evaluation feedback}\",\n",
    "  \"Suggestion\": \"{Suggestion}\"\n",
    "  \"Total_Score\" : \"{Maximum possible Score}\" \n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a critic feedback system and grader providing marks out of 100 with valid reasons why less marks are granted and grant fewer marks for wrong implementation.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{prompt} Code is as below : {code} Evaluate by requirements given: {requir} in given output format {outcome_format} + dont provide code at any point\"}\n",
    "]\n",
    "\n",
    "\n",
    "# Format input using apply_chat_template\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "model_inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate response\n",
    "generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512)\n",
    "generated_ids = [output_ids[len(model_inputs.input_ids[0]):] for output_ids in generated_ids]\n",
    "\n",
    "# Decode response\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"Generated Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1cb82b-fa13-400f-a88a-6244c49fbd2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a921f54b-e7ac-4098-9d8b-19706f2cf330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: ld.so: object '/opt/intel/oneapi/intelpython/lib/libtcmalloc.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/intel/oneapi/intelpython/lib/libtcmalloc.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: flask in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (3.0.3)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from flask) (3.0.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from flask) (3.1.3)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from flask) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from flask) (8.1.7)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /home/u3de0c7e3c41391700102d87c8bbfc17/.local/lib/python3.9/site-packages (from flask) (1.8.2)\n",
      "Requirement already satisfied: importlib-metadata>=3.6.0 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from flask) (7.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from importlib-metadata>=3.6.0->flask) (3.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from Jinja2>=3.1.2->flask) (2.1.4)\n",
      "ERROR: ld.so: object '/opt/intel/oneapi/intelpython/lib/libtcmalloc.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/intel/oneapi/intelpython/lib/libtcmalloc.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyngrok\n",
      "  Downloading pyngrok-7.1.6-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from pyngrok) (6.0.1)\n",
      "Downloading pyngrok-7.1.6-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: pyngrok\n",
      "\u001b[33m  WARNING: The scripts ngrok and pyngrok are installed in '/home/u3de0c7e3c41391700102d87c8bbfc17/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed pyngrok-7.1.6\n",
      "Enter your authtoken, which can be copied from https://dashboard.ngrok.com/get-started/your-authtoken\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "#Using Flsk to create API for this model \n",
    "!pip install flask\n",
    "!pip install pyngrok\n",
    "from flask import Flask, request, jsonify\n",
    "import re\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "import threading\n",
    "from pyngrok import ngrok, conf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26fff0b8-5313-4fca-ab06-aa8f0233e0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-23 09:37:51,589 - pyngrok.ngrok - INFO - Opening tunnel named: http-5000-b38e3ae5-9c6b-4f8d-83ce-69686a46a77d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your authtoken, which can be copied from https://dashboard.ngrok.com/get-started/your-authtoken\n",
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-23 09:37:52,827 - pyngrok.process - INFO - Overriding default auth token\n",
      "2024-06-23 09:37:52,941 - pyngrok.process.ngrok - INFO - t=2024-06-23T09:37:52+0000 lvl=info msg=\"no configuration paths supplied\"\n",
      "2024-06-23 09:37:52,943 - pyngrok.process.ngrok - INFO - t=2024-06-23T09:37:52+0000 lvl=info msg=\"using configuration at default config path\" path=/home/u3de0c7e3c41391700102d87c8bbfc17/.config/ngrok/ngrok.yml\n",
      "2024-06-23 09:37:52,944 - pyngrok.process.ngrok - INFO - t=2024-06-23T09:37:52+0000 lvl=info msg=\"open config file\" path=/home/u3de0c7e3c41391700102d87c8bbfc17/.config/ngrok/ngrok.yml err=nil\n",
      "2024-06-23 09:37:52,951 - pyngrok.process.ngrok - INFO - t=2024-06-23T09:37:52+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\n"
     ]
    },
    {
     "ename": "PyngrokNgrokError",
     "evalue": "The ngrok process was unable to start.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPyngrokNgrokError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m app \u001b[38;5;241m=\u001b[39m Flask(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Open a ngrok tunnel to the HTTP server\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m public_url \u001b[38;5;241m=\u001b[39m \u001b[43mngrok\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpublic_url\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m * ngrok tunnel \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mhttp://127.0.0.1:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(public_url, \u001b[38;5;241m5000\u001b[39m))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Update any base URLs to use the public ngrok URL\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyngrok/ngrok.py:316\u001b[0m, in \u001b[0;36mconnect\u001b[0;34m(addr, proto, name, pyngrok_config, **options)\u001b[0m\n\u001b[1;32m    312\u001b[0m             options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbasic_auth\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [auth]\n\u001b[1;32m    314\u001b[0m         options\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 316\u001b[0m api_url \u001b[38;5;241m=\u001b[39m \u001b[43mget_ngrok_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyngrok_config\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mapi_url\n\u001b[1;32m    318\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating tunnel with options: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    320\u001b[0m tunnel \u001b[38;5;241m=\u001b[39m NgrokTunnel(api_request(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapi_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/tunnels\u001b[39m\u001b[38;5;124m\"\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m, data\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    321\u001b[0m                                  timeout\u001b[38;5;241m=\u001b[39mpyngrok_config\u001b[38;5;241m.\u001b[39mrequest_timeout),\n\u001b[1;32m    322\u001b[0m                      pyngrok_config, api_url)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyngrok/ngrok.py:156\u001b[0m, in \u001b[0;36mget_ngrok_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    152\u001b[0m     pyngrok_config \u001b[38;5;241m=\u001b[39m conf\u001b[38;5;241m.\u001b[39mget_default()\n\u001b[1;32m    154\u001b[0m install_ngrok(pyngrok_config)\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyngrok_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyngrok/process.py:235\u001b[0m, in \u001b[0;36mget_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_process_running(pyngrok_config\u001b[38;5;241m.\u001b[39mngrok_path):\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _current_processes[pyngrok_config\u001b[38;5;241m.\u001b[39mngrok_path]\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_start_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyngrok_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyngrok/process.py:402\u001b[0m, in \u001b[0;36m_start_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PyngrokNgrokError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe ngrok process errored on start: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mngrok_process\u001b[38;5;241m.\u001b[39mstartup_error\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    399\u001b[0m                                 ngrok_process\u001b[38;5;241m.\u001b[39mlogs,\n\u001b[1;32m    400\u001b[0m                                 ngrok_process\u001b[38;5;241m.\u001b[39mstartup_error)\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 402\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PyngrokNgrokError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe ngrok process was unable to start.\u001b[39m\u001b[38;5;124m\"\u001b[39m, ngrok_process\u001b[38;5;241m.\u001b[39mlogs)\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ngrok_process\n",
      "\u001b[0;31mPyngrokNgrokError\u001b[0m: The ngrok process was unable to start."
     ]
    }
   ],
   "source": [
    "print(\"Enter your authtoken, which can be copied from https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
    "conf.get_default().auth_token = \"2iGty5lGP8g2jc5apF7Fe3tfL4O_2iQrhGY6ViJm7rQwGStHs\"\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Open a ngrok tunnel to the HTTP server\n",
    "public_url = ngrok.connect(5000).public_url\n",
    "print(\" * ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}/\\\"\".format(public_url, 5000))\n",
    "\n",
    "# Update any base URLs to use the public ngrok URL\n",
    "app.config[\"BASE_URL\"] = public_url\n",
    "\n",
    "# ... Update inbound traffic via APIs to use the public-facing ngrok URL\n",
    "\n",
    "@app.route('/evaluate', methods=['POST'])\n",
    "def evaluate():\n",
    "    data = request.get_json()\n",
    "\n",
    "    if not data or 'code' not in data or 'requirements' not in data:\n",
    "        return jsonify({\"error\": \"Invalid input\"}), 400\n",
    "\n",
    "    code = data['code']\n",
    "    requirements = data['requirements']\n",
    "    prompt = \"Grade my code against requirements and provide feedback for each wrong step and right step \"\n",
    "    outcome_format = \"\"\"\n",
    "    {\n",
    "      \"Score\": \"{Score}\",\n",
    "      \"Evaluation feedback\": \"{Evaluation feedback}\",\n",
    "      \"Suggestion\": \"{Suggestion}\"\n",
    "      \"Total_Score\" : \"{Maximum possible Score}\" \n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a critic feedback system and grader providing marks out of 100 with valid reasons why less marks are granted and grant fewer marks for wrong implementation.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{prompt} Code is as below : {code} Evaluate by requirements given: {requir} in given output format {outcome_format} + dont provide code at any point\"}\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    # Format input using apply_chat_template\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    model_inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate response\n",
    "    generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512)\n",
    "    generated_ids = [output_ids[len(model_inputs.input_ids[0]):] for output_ids in generated_ids]\n",
    "    \n",
    "    # Decode response\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    \n",
    "    return jsonify(response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n",
    "\n",
    "\n",
    "# Start ngrok tunnel\n",
    "public_url = ngrok.connect(5000).public_url\n",
    "print(f\" * ngrok tunnel {public_url} -> http://127.0.0.1:5000/\")\n",
    "\n",
    "# Update any base URLs to use the public ngrok URL\n",
    "app.config[\"BASE_URL\"] = public_url\n",
    "\n",
    "# Start Flask server (note: use_reloader=False to prevent it from restarting in Jupyter notebooks)\n",
    "app.run(use_reloader=False)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch GPU",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
